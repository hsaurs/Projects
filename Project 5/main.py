# -*- coding: utf-8 -*-
"""main

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r02WmV2KcXMsp5237V6R41yVg6zB-xQL
"""

from google.colab import files
myfile = files.upload()

import pandas as pd
import numpy as np
import re
from konlpy.tag import Okt
from collections import Counter

df = pd.read_excel("잡플래닛 리뷰 총정리.xlsx")

# 별점, 장점, 단점만 사용
df = df[['별점','장점','단점']]

#  '점' 제거
for i in range(len(df['별점'])):
    df['별점'][i] = df['별점'][i].replace("점","")

# 정규표현식 - 한글 및 공백만 사용
df['장점'] = df['장점'].str.replace('[^ ㄱ-ㅣ가-힣+]','')
df['단점'] = df['단점'].str.replace('[^ ㄱ-ㅣ가-힣+]','')

stopwords = ['은','는','이','가','로','좀','와','를','음','있다','으로','에','히','하다','의','고','지','것','수','등등','셋','안','함','님','보지','격','및','비','중','왜','티','타','주','점','때문','경우','그','대한']

def tokenizer(x):
    preprocessed_sentences = []
    for sentence in x:
	    tokenized_sentence = okt.nouns(sentence) # 텍스트에서 명사를 반환(오픈소스 한국어 분석기)
	    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제외
	    preprocessed_sentences.append(stopwords_removed_sentence)
    return preprocessed_sentences

# tokenizer 적용
pros = tokenizer(df['장점'])
cons = tokenizer(df['단점'])

# 모든 단어 결합
pros = sum(pros, []) 
cons = sum(cons, [])

# Collection 모듈의 Counter 클래스를 통해 Key-value 형태로 변환
pros = Counter(pros) 
cons = Counter(cons)

# 상위 10개 추출
pros = pros.most_common(10) 
cons = cons.most_common(10) 
print(pros) 
print(cons)

#dictionary 에서 Dataframe 형태로 변환

pros = pd.DataFrame.from_dict(dict(pros), orient='index')
cons = pd.DataFrame.from_dict(dict(cons), orient='index')